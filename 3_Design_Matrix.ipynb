{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Matrices and Evaluating Individual Regressors\n",
    "\n",
    "In the previous notebook, we learned how to fit a linear encoding model with ridge regression. Ultimately, we would like to understand what variables have the strongest influence on neural activity. We can do that by looking at the $\\beta$ values for individual regressors, as we did in the last notebook. We can also compute the variance explained ($R^2$) for a particular variable. \n",
    "\n",
    "Here, we will learn how to compute the variance explained ($R^2$) by individual task variables, while applying the functions we wrote previously in `2_Fitting_Neural_Data.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data, same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/churchlandlab/DataSAI_summer_school/blob/main/3_Design_Matrix.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "!pip install gdown scikit-learn tqdm pandas matplotlib\n",
    "!pip install git+https://github.com/churchlandlab/DataSAI_summer_school.git # Install a custom package for these notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import encoding_tools\n",
    "\n",
    "data_path = encoding_tools.download_neural_data(\"miniscope\") #Download data\n",
    "design_matrix, Y_raw_fluorescence, neuron_footprints, frames_per_trial, frame_rate, aligned_segment_start = encoding_tools.load_miniscope(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing alpha values and $R^2$\n",
    "For this notebook, we will import the functions `find_best_alphas()` and `cross_val_ridge()`. These functions will handle the operations that we coded in the previous notebook:\n",
    "\n",
    "- `find_best_alphas()` will do a grid-search to find the optimal alpha values\n",
    "- `cross_val_ridge()` will take those alphas and return a cross-validated $R^2$ for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show function documentation\n",
    "??encoding_tools.find_best_alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??encoding_tools.cross_val_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are going to run these functions on the full design matrix, which includes task variables (such as stimulus strength and the mouse's decision), head orientation, and video tracking with DeepLabCut. We will then plot the variance explained for each neuron recorded in this seession. The output should be very similar to what we saw in the last notebook, because we are fitting our model with the same design matrix $X$ and the same neural data $Y$.\n",
    "\n",
    "**In the following cell, call the functions defined above to perform the ridge regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "Y = zscore(Y_raw_fluorescence, axis=0)\n",
    "scaler_obj = encoding_tools.standardize_x_cols(column_idx=np.arange(832, 1258)).fit(design_matrix) #Cols 0:827 are task vars, cols 828:832 are head orientation\n",
    "X = scaler_obj.transform(design_matrix)\n",
    "\n",
    "print(f'Shape of x is {X.shape}')\n",
    "print(f'Shape of y is {Y.shape}')\n",
    "\n",
    "N_FOLDS = 10\n",
    "alpha_test_range =  10**np.linspace(-2,6,9) \n",
    "print('Computing alphas...')\n",
    "\n",
    "best_alphas = #EXERCISE: call encoding_tools.find_best_alphas with the proper parameters\n",
    "\n",
    "print('Computing cross-validated scores...')\n",
    "\n",
    "r_squared_scores, betas = #EXERCISE: call encoding_tools.cross_val_ridge with the proper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot $R^2$ averaged across folds for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot R^2 averaged across folds for each neuron\n",
    "average_scores = np.mean(r_squared_scores, axis=0)\n",
    "plt.plot(np.sort(average_scores)[::-1], color='black')\n",
    "plt.xlabel('Neuron')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Full model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the variance explained is similar to what we saw before. This plot gives us a nice overview of how much variance we can explain with our full model (meaning that we have included as many predictors as we can think of). Unfortunately this data is not very interpretable: we don't actually know **what** variables the neurons are most strongly encoding.\n",
    "\n",
    "To understand what individual neurons are most strongly encoding, we will need to train **single variable models**, which contain shuffled versions of the design matrix $X$, where we randomly shuffle regressors that are not of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Compute the variance explained solely by choice\n",
    "To compute the variance explained by choice, we need to manipulate our design matrix. We will do that by randomly shuffling all of the columns within our design matrix, except for the columns that contain the choice regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_design_matrix(design_matrix, shuffle_inds):\n",
    "    \"\"\"\n",
    "    EXERCISE: write a function that will shuffle the regressors in the design_matrix specified by shuffle_inds.\n",
    "    (if shuffle_inds = np.array([0,4,5]), then regressors at index 0,4, and 5 of the design matrixshould be randomly shuffled along the time axis).\n",
    "    Make sure that each regressor is shuffled independently of the others, then return the shuffled design matrix\n",
    "    \"\"\"\n",
    "    return design_matrix\n",
    "\n",
    "n_regressors = design_matrix.shape[1]\n",
    "shuffle_inds = np.hstack((np.arange(0, frames_per_trial+1), np.arange(2*frames_per_trial+1, n_regressors)))\n",
    "\n",
    "design_matrix_choice_only = shuffle_design_matrix(np.array(design_matrix), shuffle_inds) # shuffle everything but choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the regression with the shuffled design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_obj = encoding_tools.standardize_x_cols(column_idx=np.arange(832, 1258)).fit(design_matrix_choice_only)\n",
    "X = scaler_obj.transform(design_matrix_choice_only)\n",
    "\n",
    "print('Computing alphas...')\n",
    "best_alphas = encoding_tools.find_best_alphas(X, Y, alpha_test_range) # this will take a second to run\n",
    "print('Computing cross-validated scores...')\n",
    "r_squared_scores_choice, _ = encoding_tools.cross_val_ridge(X, Y, best_alphas, n_folds=N_FOLDS) # this will take a second to run\n",
    "\n",
    "average_scores_choice = np.mean(r_squared_scores_choice, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the variance explained by choice\n",
    "Here, we are going to sort the neurons on our plot by the amount of variance we can explain with the full model. We will plot the \"full model\" variance explained, and the variance explained by the \"choice\" task variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot R^2 averaged across folds for each neuron\n",
    "sorting_indices = np.argsort(average_scores)[::-1]\n",
    "example_neuron_idx = [np.where(sorting_indices==56)[0][0], np.where(sorting_indices==12)[0][0]]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "plt.plot(average_scores[sorting_indices], color='black', label='Full model', linewidth=.8)\n",
    "plt.plot(average_scores_choice[sorting_indices], color='blue', label='Choice', linewidth=.8)\n",
    "ax.scatter(example_neuron_idx, average_scores_choice[[56,12]], color = 'red')\n",
    "plt.xlabel('Neuron')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Full model and choice only model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a clearer picture of how strongly choice is encoded across our population of neurons\n",
    "Take note of our example neurons labeled in red. It seems one of them more strongly encodes choice than the other. Does this match what we saw in `2_Fitting_Neural_Data.ipynb`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing unique variance explained, $\\Delta R^2$\n",
    "We just learned how to compute the variance explained ($R^2$) for an individual variable. We can also compute the *unique variance explained* ($\\Delta R^2$) by a particular variable. \n",
    "\n",
    "Figure 4 from [Musall *et al* 2019](https://www.nature.com/articles/s41593-019-0502-4) provides a nice illustration of unique variance explained.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/delta_r_squared.PNG\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "As we discussed in `1_Ridge_Regression.ipynb`, the whole reason we are using ridge regression to solve these systems of equations is because regressors are often correlated with each other. Since ridge regression will spread variance across correlated regressors, each individual regressor will explain some amount of *redundant* variance (variance explainable by other regressors) and some amount of *unique* variance (variance only explainable by that particular regressor). \n",
    "\n",
    "- $R^2$ (what we just computed for choice) includes the redundant variance and the unique variance.\n",
    "- $\\Delta R^2$ includes just the unique variance. It is a more conservative metric.\n",
    "\n",
    "\n",
    "To compute $\\Delta R^2$, we will first compute the variance of the full model (which we did previously). We will then compute the variance explained on a model where the variable of interest has been shuffled. The $\\Delta R^2$ is the difference in variance explained between these two models.\n",
    "\n",
    "To compute the $\\Delta R^2$ for choice:\n",
    "\n",
    "$$\n",
    "\\Delta R^2_{\\text{choice}} = R^2_{\\text{full model}} - R^2_{\\text{choice shuffle}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already computed $R^2_{\\text{full model}}$, so let's compute $R^2_{\\text{choice shuffle}}$ and then get the $\\Delta R^2$.\n",
    "\n",
    "## Exercise: compute the unique variance explained for choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_regressors = design_matrix.shape[1]\n",
    "shuffle_inds = np.arange(frames_per_trial+1, 2*frames_per_trial+1)\n",
    "design_matrix_choice_shuffled = shuffle_design_matrix(np.array(design_matrix), shuffle_inds)  # shuffle ONLY choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the regression\n",
    "**In the following cell, you will need to fill in one line to compute choice_delta_r2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_obj = encoding_tools.standardize_x_cols(column_idx=np.arange(832, 1258)).fit(design_matrix_choice_shuffled)\n",
    "X = scaler_obj.transform(design_matrix_choice_shuffled)\n",
    "\n",
    "print('Computing alphas...')\n",
    "best_alphas = encoding_tools.find_best_alphas(X, Y, alpha_test_range) # this will take a second to run\n",
    "print('Computing cross-validated scores...')\n",
    "r_squared_scores_choice_shuffled, _ = encoding_tools.cross_val_ridge(X, Y, best_alphas, n_folds=N_FOLDS) # this will take a second to run\n",
    "\n",
    "average_scores_choice_shuffled = np.mean(r_squared_scores_choice_shuffled, axis=0) #average over folds\n",
    "\n",
    "choice_delta_r2 = #EXERCISE: compute deltaR2 for choice here, using the two models we have now trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the **unique** variance explained by choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot R^2 averaged across folds for each neuron\n",
    "sorting_indices = np.argsort(average_scores_choice)[::-1]\n",
    "example_neuron_idx = [np.where(sorting_indices==56)[0][0], np.where(sorting_indices==12)[0][0]]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "plt.plot(average_scores_choice[sorting_indices], color='blue', label='Choice $R^2$', linewidth=.8)\n",
    "plt.plot(choice_delta_r2[sorting_indices], color='green', label='Choice $\\Delta R^2$', linewidth=.8)\n",
    "ax.scatter(example_neuron_idx, choice_delta_r2[[56,12]], color = 'red')\n",
    "plt.xlabel('Neuron')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Full model and choice only model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, $\\Delta R^2$ values tend to be much lower than $R^2$!\n",
    "\n",
    "### Congrats on finishing the final notebook! You now know how to compute and extract the three basic metrics from a linear encoding model\n",
    "- $\\beta$ weights\n",
    "- $R^2$\n",
    "- $\\Delta R^2$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
